{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43a4a59",
   "metadata": {},
   "source": [
    "<table align=\"left\" width=100%>\n",
    "    <tr>\n",
    "        <td width=\"15%\">\n",
    "            <img src=\"faculty.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <div align=\"center\">\n",
    "                <font color=\"#21618C\" size=8px>\n",
    "                  <b> Faculty Notebook <br> Session 1 :NLP Introduction and Basics of Text Pre-processing  </b>\n",
    "                </font>\n",
    "            </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be495b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table of Contents\n",
    "1.  **[Text Standardization](#TextStandardization)**\n",
    "    - 1.1 [Case Conversion](#CaseConversion)\n",
    "    - 1.2 [Spelling Correction](#SpellingCorrection)<br></br>    \n",
    "2.  **[Text Normalization ](#TextNormalization)**\n",
    "    - 2.1 [Stemming](#Stemming)\n",
    "    - 2.2 [Parts Of Speech(POS) tagging](#POStagging)\n",
    "    - 2.3 [Lemmatization](#Lemmatization)<br></br>   \n",
    "3.  **[Eliminate Unessential Items from Text ](#TextElimination)**\n",
    "    - 3.1 [Removing Spaces](#RemovingSpaces)\n",
    "    - 3.2 [Removing Digits](#RemovingDigits)\n",
    "    - 3.3 [Removing Stopwords](#RemovingStopwords) \n",
    "    - 3.4 [Removing Punctuations](#RemovingPunctuations)\n",
    "    - 3.5 [Removing URLs](#RemovingHTMLTags) \n",
    "    - 3.6 [Removing Accented Characters](#RemovingAccentedCharacters)<br></br>   \n",
    "4.  **[Working with Emoji](#WorkingwithEmoji)**<br></br>\n",
    "5.  **[Web Scrapping: Text Extraction from web Page](#WebScrapping)**<br></br> \n",
    "6.  **[Text Extraction](#TextExtraction)**\n",
    "    - 6.1 [Extracting Text from PDF File](#TextExtractionformPDF)\n",
    "    - 6.2 [Extracting Text from IMAGE File](#TextExtractionformImages)<br></br>\n",
    "7.  **[Name Entity Recognition (NER)](#NER)**<br></br>\n",
    "8.  **[Dependancy Parcing](#DependancyParcing)**<br></br>\n",
    "9.  **[Word Cloud](#WordCloud)**<br></br> \n",
    "10. **[Sentiment Analysis using Textblob](#SentimentAnalysis)**<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e24a1",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0fc09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"TextStandardization\"> </a>\n",
    "### 1. Text Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957bd18",
   "metadata": {},
   "source": [
    "<a id=\"CaseConversion\"> </a>\n",
    "#### 1.1. Case Conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c7f161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' lets start learning nlp'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to lower case \n",
    "text = \" Lets Start learning NLP\"\n",
    "# python inbuild function\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea4ee4",
   "metadata": {},
   "source": [
    "<a id=\"SpellingCorrection\"> </a>\n",
    "#### 1.2. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558cdf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (0.8.3)\n",
      "this is spelling correct test\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "text = \"this is speling correct tst\"\n",
    "print (correct_spellings(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ac3ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"TextNormalization\"> </a>\n",
    "### 2. Text Normalization  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abeead",
   "metadata": {},
   "source": [
    "<a id=\"Stemming\"> </a>\n",
    "#### 2.1 Stemming  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9e2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  from scipy.stats import fisher_exact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6be09d",
   "metadata": {},
   "source": [
    "<a id=\"POStagging\"> </a>\n",
    "#### 2.2 Parts Of Speech(POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f225158-c75a-4296-a58f-ca8de3e3896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('tagsets_json')\n",
    "#nltk.download('RB')\n",
    "#nltk.download('NNP')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2960a2f-e88d-4269-86b2-a854ddd96dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')          # for tokenizing sentences\n",
    "nltk.download('wordnet')        # for lemmatization\n",
    "nltk.download('stopwords')      # for removing stopwords\n",
    "nltk.download('tagsets')        # for understanding POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad56bf-e702-480e-a35e-5ea873a4f1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58082dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('first', 'RB'),\n",
       " ('lecture.', 'VBD')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using nltk library  \n",
    "import nltk\n",
    "\n",
    "text = \" You are learning NLP first lecture.\"\n",
    "nltk.pos_tag(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ece2fa-5fbd-4407-b6a6-330d36eb9166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703cb855-0d2d-4bb6-a14e-275edcc7fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8626eca2-5af3-4f3e-b3af-3c022d7c9915",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2174361826.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m spacy download en_core_web_sm\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8bf47db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load models in memory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Process a sentence\u001b[39;00m\n\u001b[0;32m      4\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShe saw a bear. She was very afraid.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load models in memory\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#Process a sentence\n",
    "doc = nlp('She saw a bear. She was very afraid.')\n",
    "#Print POS tags for each word\n",
    "for word in doc:\n",
    "    print(word, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ca015-dd86-48c0-a63d-9b13e53dd10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf21733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe of POS \n",
    "import pandas as pd \n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in doc]\n",
    "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03040d21",
   "metadata": {},
   "source": [
    "<a id=\"Lemmatization\"> </a>\n",
    "#### 2.3 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6820446-3bf5-4bef-a9c8-28474c85c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of each tag \n",
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization without POS specification \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "  \n",
    "\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])    \n",
    "\n",
    "\n",
    "lemmatize_text(\"David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization with POS specification \n",
    "# import these modules \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# without wordnet map it takes evey word as noun\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\n",
    "\n",
    " \n",
    "def lemma_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "text = \"David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo \"\n",
    "lemma_words(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4954d2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"TextElimination\"> </a>\n",
    "### 3. Eliminate Unessential Items from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa79db",
   "metadata": {},
   "source": [
    "<a id=\"RemovingSpaces\"> </a>\n",
    "#### 3.1 Removing Spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing multiple spaces\n",
    "#Comverting line with mutiple Spaces into line with single space b/w words\n",
    "import re\n",
    "text = \"Converting line   with    many   spaces to     line with single space between words.\"\n",
    "text = re.sub(' +',' ',text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734d365",
   "metadata": {},
   "source": [
    "<a id=\"RemovingDigits\"> </a>\n",
    "#### 3.2 Removing Digits \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"Being no 1 in exam is more important or being no 3   with fair ways \"\n",
    "text= re.sub(r'[0-9]','',text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3e142",
   "metadata": {},
   "source": [
    "<a id=\"RemovingStopwords\"> </a>\n",
    "#### 3.3 Removing Stopwords \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e1582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "#nltk.download('stopwords')\n",
    "text = \"Stoword is one if the important topic\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # stop_words will contain  set all english stopwords\n",
    "    filtered_sentence = []   \n",
    "    for word in text.split(): \n",
    "        if word not in stop_words: \n",
    "            filtered_sentence.append(word) \n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a0b68",
   "metadata": {},
   "source": [
    "<a id=\"RemovingPunctuations\"> </a>\n",
    "#### 3.4 Removing Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This! sentence, contains so: many - punctuations.\"\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f86422",
   "metadata": {},
   "source": [
    "<a id=\"RemovingHTMLTags\"> </a>\n",
    "#### 3.5 Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf71f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Shall I search the answer in www.google.com ?'\n",
    "text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185e9b9",
   "metadata": {},
   "source": [
    "<a id=\"RemovingAccentedCharacters\"> </a>\n",
    "#### 3.6  Removing Accented Characters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "remove_accented_chars('S√≥mƒõ √Åccƒõntƒõd tƒõxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0d3ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"WorkingwithEmoji\"> </a>\n",
    "### 4. Working with Emoji (*optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ca2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#input data\n",
    "input_text = 'He is üò≥'\n",
    "\n",
    "#Replace emoji icon with text\n",
    "output_text = emoji.demojize(input_text)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove ':' from emoji text\n",
    "output_text = output_text.replace(':','')\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e62f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "<a id=\"WebScrapping\"> </a>\n",
    "## 5. Web Scrapping: Text Extraction from web Page\n",
    "\n",
    "- Downloding web data from websites \n",
    "- Parse html/Web data\n",
    "- Read the data in a Dataframe\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb637e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting text from websites\n",
    "import requests\n",
    "#Let's download the data from inshorts website. In this case, news articles will be from 'technology' category\n",
    "url = 'https://inshorts.com/en/read/technology'\n",
    "#Download the data from Website\n",
    "data = requests.get(url)\n",
    "# show\n",
    "print(data.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9991a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Cleaning -  Cleaning the extacted data \n",
    "# use Beautiful Soup package to parse  html/Web data\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(data.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb55b53",
   "metadata": {},
   "source": [
    "Now we will Read all the articles. \n",
    "For each article, we will read:\n",
    "- Headline\n",
    "- Article body\n",
    "- Category\n",
    "\n",
    "This is done by reading text between specific HTML tags. The tags depend on actual web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = []\n",
    "news_category = url.split('/')[-1]\n",
    " \n",
    "news_articles = [{'news_headline': headline.find('span', attrs={'itemprop': 'headline'}).string,\n",
    "                  'news_article': article.find('div', attrs={'itemprop': 'articleBody'}).string,\n",
    "                  'news_category': news_category} \n",
    "for headline, article in zip(soup.find_all('div', class_ = ['news-card-title news-right-box']), \n",
    "                             soup.find_all('div', class_=['news-card-content news-right-box']))]\n",
    "\n",
    "#Check news data\n",
    "news_data.extend(news_articles)\n",
    "news_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d573bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read the news data in a Dataframe\n",
    "import pandas as pd\n",
    "#Building dataframe\n",
    "df = pd.DataFrame(news_data, columns=['news_headline', 'news_article', 'news_category'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5c36a",
   "metadata": {},
   "source": [
    "<a id=\"TextExtraction\"> </a>\n",
    "## 6. Text Extraction form different file types (PDF/Images) (*optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e75ff",
   "metadata": {},
   "source": [
    "<a id=\"TextExtractionformPDF\"> </a>\n",
    "#### 6.1. Extracting Text from PDF File (*optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "761e455b-f8e6-4e21-9676-c872bbba715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     ------------------------------------- -- 3.9/4.2 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.2/4.2 MB 20.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pycryptodome (from pdfminer)\n",
      "  Downloading pycryptodome-3.23.0-cp37-abi3-win_amd64.whl.metadata (3.5 kB)\n",
      "Downloading pycryptodome-3.23.0-cp37-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 16.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py): started\n",
      "  Building wheel for pdfminer (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140760 sha256=1a166ae91a02f4337c52f2c9d78029d669a990b1b232e74e916bad51604f59d3\n",
      "  Stored in directory: c:\\users\\manju\\appdata\\local\\pip\\cache\\wheels\\90\\7b\\26\\62139fb7c8c5c242c492e02ce8613ca4c3df4cd86afb8e6264\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db775283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,  and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.  Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data delug \\x0c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "import io\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# function to process each of the pdf document (path in input) and return a string (text)\n",
    "def pdf_to_text(path):\n",
    "    manager = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layout = LAParams(all_texts=True)\n",
    "    device = TextConverter(manager, retstr)\n",
    "    \n",
    "    filepath = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "    for page in PDFPage.get_pages(filepath, caching=True,check_extractable=True):\n",
    "        #print(page[0])\n",
    "        interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "    filepath.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "pdf_to_text('text_as_pdf.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4de63",
   "metadata": {},
   "source": [
    "<a id=\"TextExtractionformImages\"> </a>\n",
    "#### 6.2. Extracting Text from IMAGE File (*optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f54ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: Below code require some additional librarier and may not run in all system\n",
    "# !pip install pytesseract\n",
    "# !pip install tesseract\n",
    "# ! pip install tesseract-ocr\n",
    "# for mac: brew install tesseract\n",
    "# for windows : download binary from https://github.com/UB-Mannheim/tesseract/wiki. then add pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe'\n",
    "# for linux : sudo apt-get install libleptonica-dev tesseract-ocr tesseract-ocr-dev libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "image_path_in_colab='text_as_image.jpg'\n",
    "extractedInformation = pytesseract.image_to_string(Image.open(image_path_in_colab))\n",
    "print(extractedInformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e814cbe",
   "metadata": {},
   "source": [
    "<a id=\"NER\"> </a>\n",
    "### 7. Name Entity Recognition (NER)  \n",
    "\n",
    "- Detecting the Entities from the text and Classifying Entities into different categories\n",
    "- NER usage example \n",
    "    - Topic Modeling : understand text is by analyzing its topics\n",
    "    - Text Summarization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76cae7f-1c23-4327-8425-25e233dc9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy is prefered for NER\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "text = '''European authorities fined Google a record $5.1 billion on Wednesday for \n",
    "abusing its power in the mobile phone market and ordered the company to alter its practices'''\n",
    "\n",
    "doc = nlp(text)\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e467709",
   "metadata": {},
   "source": [
    "European is NORP (nationalities or religious or political groups), Google is an organization, $5.1 billion is monetary value and Wednesday is a date object. They are all correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0646c",
   "metadata": {},
   "source": [
    "#### NER usage : \n",
    "- Fetching sentences\n",
    "- Fetching tokens\n",
    "- Extracting named entity from an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and load spacy english language package\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy import tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text ='''Python is an interpreted, high-level and general-purpose programming language \n",
    "       Pythons design philosophy emphasizes code readability with\"\n",
    "¬†¬†¬†¬†¬†¬†¬†its notable use of significant indentation.\"\n",
    "¬†¬†¬†¬†¬†¬†¬†Its language constructs and object-oriented approach aim to\"\n",
    "¬†¬†¬†¬†¬†¬†¬†help programmers write clear and\"\n",
    "¬†¬†¬†¬†¬†¬†¬†logical code for small and large-scale projects'''\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f11e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching sentences\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe597b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fetching tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6de020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use display function on doc\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bfa32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = '''Barack Obama is an American politician who served as the 44th President of the United States \n",
    "from 2009 to 2017.He is the first African American to have served as president, \n",
    "as well as the first born outside the contiguous United States. He speaks English.'''\n",
    "\n",
    "doc3 = nlp(text)\n",
    "displacy.render(doc3, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cbb09",
   "metadata": {},
   "source": [
    "<a id=\"DependancyParcing\"> </a>\n",
    "### 8. Dependancy Parcing \n",
    "\n",
    "- Process of analyzing grammatical structure in a sentence and find out related words as well & type of the relationship between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdb3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualize dependecy parsing \n",
    "from spacy import displacy\n",
    "#For 1st sentence\n",
    "doc = nlp('She saw a bear. She was very afraid.')\n",
    "# doc\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42738702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second sentence\n",
    "displacy.render(doc3, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82360382",
   "metadata": {},
   "source": [
    "<a id=\"WordCloud\"> </a>\n",
    "### 9. Word Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install wordcloud \n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# import the wordcloud library\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "# Instantiate a new wordcloud.\n",
    "wordcloud = WordCloud(random_state = 8,\n",
    "        normalize_plurals = False,\n",
    "        width = 600, height= 300,\n",
    "        max_words = 300,\n",
    "#          background_color='white',  \n",
    "        stopwords = stopwords)\n",
    "\n",
    "# Apply the wordcloud to the text.\n",
    "text = '''Barack Obama is an American politician who served as the 44th President of the United States \n",
    "from 2009 to 2017.He is the first African American to have served as president, \n",
    "as well as the first born outside the contiguous United States. He speaks English.'''\n",
    "\n",
    "wordcloud.generate(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7aafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting wordcloud \n",
    "import matplotlib.pyplot as plt\n",
    "# create a figure\n",
    "fig, ax = plt.subplots(1,1, figsize = (9,6))\n",
    "# add interpolation = bilinear to smooth things out\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# and remove the axis\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add6613",
   "metadata": {},
   "source": [
    "<a id=\"SentimentAnalysis\"> </a>\n",
    "### 10. Sentiment Analysis using Textblob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c93bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I hate anything that goes in my ear\"\n",
    "textblob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching text sentiment polarity \n",
    "textblob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ea808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching text sentiment subjectivity\n",
    "textblob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for analysis sentiment\n",
    "# def getTextAnalysis(a):\n",
    "#     if a < 0:\n",
    "#         return \"Negative\"\n",
    "#     elif a == 0:\n",
    "#         return \"Neutral\"\n",
    "#     else:\n",
    "#         return \"Positive\" \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
