import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import ResNet50, VGG16 # Example pre-trained models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint




# --- 1. Configuration and Setup ---
# Define paths
DATASET_DIR = 'dataset'
TRAIN_REAL_DIR = os.path.join(DATASET_DIR, 'training_real')
TRAIN_FAKE_DIR = os.path.join(DATASET_DIR, 'training_fake')
TEST_DIR = 'test' # Sibling to dataset folder

TRAIN_CSV_PATH = 'train.csv'
TEST_CSV_PATH = 'test.csv'

# Image parameters
IMG_HEIGHT = 128
IMG_WIDTH = 128
BATCH_SIZE = 32
NUM_CLASSES = 2 # Real and Fake
EPOCHS = 20 # You might need to adjust this based on convergence
LEARNING_RATE = 0.001



# Load train.csv
try:
    train_df = pd.read_csv(TRAIN_CSV_PATH)
    print(f"train_df head:\n{train_df.head()}")
    print(f"train_df info:\n{train_df.info()}")
    print(f"train_df label counts:\n{train_df['label'].value_counts()}")
except FileNotFoundError:
    print(f"Error: {TRAIN_CSV_PATH} not found. Please ensure the CSV file is in the correct directory.")
    exit()

# Load test.csv
try:
    test_df = pd.read_csv(TEST_CSV_PATH)
    print(f"\ntest_df head:\n{test_df.head()}")
    print(f"test_df info:\n{test_df.info()}")
except FileNotFoundError:
    print(f"Error: {TEST_CSV_PATH} not found. Please ensure the CSV file is in the correct directory.")
    exit()






# Create a full path column for training images in train_df
# Assuming file_id in train.csv corresponds to the image name (e.g., '0.jpg')
# And assuming '0' is 'real' and '1' is 'fake' based on common practice for binary classification
# We need to map file_id to the correct subdirectory (training_real or training_fake)
# Since the train.csv only gives file_id and label, and not the source folder,
# we need to infer the folder structure.
# A more robust approach would be to glob all images and then match with train_df.

# Let's list all files in training_real and training_fake and create a combined DataFrame
# and then merge with train_df.

# Function to get file paths and corresponding labels
def get_image_paths_and_labels(base_dir, label_mapping):
    data = []
    for label_name, label_id in label_mapping.items():
        folder_path = os.path.join(base_dir, label_name)
        if not os.path.exists(folder_path):
            print(f"Warning: Directory {folder_path} not found. Please check your dataset structure.")
            continue
        for img_name in os.listdir(folder_path):
            if img_name.endswith(('.jpg', '.jpeg', '.png')): # Filter for image files
                data.append({'file_id': os.path.splitext(img_name)[0], 'file_path': os.path.join(folder_path, img_name), 'label': label_id})
    return pd.DataFrame(data)







# Assuming 'training_real' corresponds to label 0 and 'training_fake' to label 1
label_mapping = {'training_real': 0, 'training_fake': 1}
all_train_images_df = get_image_paths_and_labels(DATASET_DIR, label_mapping)

# Convert file_id to string for merging (if train_df's file_id is int)
train_df['file_id'] = train_df['file_id'].astype(str)
all_train_images_df['file_id'] = all_train_images_df['file_id'].astype(str)

# Merge with the provided train_df to ensure correct labels based on train.csv
# This handles cases where train.csv might contradict the folder structure (unlikely but good for robustness)
merged_train_df = pd.merge(all_train_images_df[['file_id', 'file_path']], train_df, on='file_id', how='inner')
merged_train_df['label'] = merged_train_df['label'].astype(str) # Convert labels to string for flow_from_dataframe

print(f"\nMerged Train DataFrame head:\n{merged_train_df.head()}")
print(f"Merged Train DataFrame info:\n{merged_train_df.info()}")
print(f"Merged Train DataFrame label counts:\n{merged_train_df['label'].value_counts()}")







if merged_train_df.empty:
    print("Error: Merged training DataFrame is empty. Check file_id matching between CSV and image filenames.")
    exit()

# Split training data for validation
train_df_split, val_df_split = train_test_split(merged_train_df, test_size=0.2, random_state=42, stratify=merged_train_df['label'])

print(f"\nTrain split size: {len(train_df_split)}")
print(f"Validation split size: {len(val_df_split)}")
print(f"Train split label counts:\n{train_df_split['label'].value_counts()}")
print(f"Validation split label counts:\n{val_df_split['label'].value_counts()}")







# Image Data Generators with Augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)



val_datagen = ImageDataGenerator(rescale=1./255) # No augmentation for validation
test_datagen = ImageDataGenerator(rescale=1./255) # No augmentation for test





train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df_split,
    x_col='file_path',
    y_col='label',
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary', # Since we have 2 classes
    shuffle=True
)

validation_generator = val_datagen.flow_from_dataframe(
    dataframe=val_df_split,
    x_col='file_path',
    y_col='label',
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False # No need to shuffle validation data
)





# Prepare test data for prediction
# Create full paths for test images
test_df['file_path'] = test_df['file_id'].apply(lambda x: os.path.join(TEST_DIR, str(x) + '.jpg')) # Assuming .jpg extension for test images

# Verify if test image files exist
# This helps in debugging if flow_from_dataframe fails
missing_test_files = [path for path in test_df['file_path'] if not os.path.exists(path)]
if missing_test_files:
    print(f"\nWarning: {len(missing_test_files)} test files not found. First 5 missing: {missing_test_files[:5]}")
    # You might want to filter these out or handle them
    test_df = test_df[test_df['file_path'].apply(os.path.exists)]
    if test_df.empty:
        print("Error: No valid test images found after filtering missing files.")
        exit()
else:
    print("\nAll test files found.")





test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='file_path',
    y_col=None, # No labels for test set
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode=None, # Important for prediction
    shuffle=False
)





# --- 3. Model Definition: From Scratch ---
def create_scratch_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Conv2D(64, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Conv2D(128, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.25),

        Flatten(),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(1, activation='sigmoid') # Sigmoid for binary classification
    ])
    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model





# --- 4. Model Definition: Pre-trained (Transfer Learning) ---

def create_pretrained_model(base_model_name='ResNet50', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):
    if base_model_name == 'ResNet50':
        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    elif base_model_name == 'VGG16':
        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)
    else:
        raise ValueError(f"Unsupported base model: {base_model_name}. Choose 'ResNet50' or 'VGG16'.")

    # Freeze the layers of the base model
    base_model.trainable = False

    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(1, activation='sigmoid') # Sigmoid for binary classification
    ])

    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model


# --- 5. Training and Evaluation ---

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model_checkpoint_scratch = ModelCheckpoint('best_scratch_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')
model_checkpoint_pretrained = ModelCheckpoint('best_pretrained_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')






# --- Model 1: From Scratch ---
print("\n--- Training Model From Scratch ---")
scratch_model = create_scratch_model()
scratch_model.summary()

history_scratch = scratch_model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping, model_checkpoint_scratch]
)

print("\n--- Evaluating Scratch Model ---")
# Load the best saved model for evaluation
best_scratch_model = tf.keras.models.load_model('best_scratch_model.keras')
loss_scratch, accuracy_scratch = best_scratch_model.evaluate(validation_generator)
print(f"Scratch Model Validation Loss: {loss_scratch:.4f}")
print(f"Scratch Model Validation Accuracy: {accuracy_scratch:.4f}")





# --- Plot Training History (Scratch Model) ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history_scratch.history['accuracy'], label='Train Accuracy')
plt.plot(history_scratch.history['val_accuracy'], label='Validation Accuracy')
plt.title('Scratch Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_scratch.history['loss'], label='Train Loss')
plt.plot(history_scratch.history['val_loss'], label='Validation Loss')
plt.title('Scratch Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()





# --- Model 2: Pre-trained (ResNet50) ---
print("\n--- Training Pre-trained Model (ResNet50) ---")
# Reset generators for pre-trained model (especially if batching/shuffling matters)
train_generator.reset()
validation_generator.reset()

pretrained_model = create_pretrained_model(base_model_name='ResNet50')
pretrained_model.summary()

history_pretrained = pretrained_model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping, model_checkpoint_pretrained]
)

print("\n--- Evaluating Pre-trained Model ---")
# Load the best saved model for evaluation
best_pretrained_model = tf.keras.models.load_model('best_pretrained_model.keras')
loss_pretrained, accuracy_pretrained = best_pretrained_model.evaluate(validation_generator)
print(f"Pre-trained Model Validation Loss: {loss_pretrained:.4f}")
print(f"Pre-trained Model Validation Accuracy: {accuracy_pretrained:.4f}")





# --- Plot Training History (Pre-trained Model) ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history_pretrained.history['accuracy'], label='Train Accuracy')
plt.plot(history_pretrained.history['val_accuracy'], label='Validation Accuracy')
plt.title('Pre-trained Model (ResNet50) Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_pretrained.history['loss'], label='Train Loss')
plt.plot(history_pretrained.history['val_loss'], label='Validation Loss')
plt.title('Pre-trained Model (ResNet50) Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()





# --- 6. Prediction on Test Set ---

print("\n--- Making Predictions on Test Set ---")

# Ensure test_generator is reset before prediction
test_generator.reset()

# Predictions from Scratch Model
print("\nPredicting with Scratch Model...")
scratch_predictions = best_scratch_model.predict(test_generator)
scratch_predicted_classes = (scratch_predictions > 0.5).astype(int) # Convert probabilities to binary classes

# Predictions from Pre-trained Model
print("Predicting with Pre-trained Model...")
pretrained_predictions = best_pretrained_model.predict(test_generator)
pretrained_predicted_classes = (pretrained_predictions > 0.5).astype(int)

# Create submission files (assuming 'file_id' from test_df needs to be mapped to predictions)
# We need to map the predictions back to the original file_ids.
# test_generator.filenames contains the relative paths, we need to extract file_id.

# Get the mapping of indices to original filenames (file_id.jpg) from the generator
test_filenames = test_generator.filenames
test_file_ids = [os.path.splitext(os.path.basename(f))[0] for f in test_filenames]

# Create submission DataFrame for Scratch Model
submission_scratch_df = pd.DataFrame({
    'file_id': test_file_ids,
    'label': scratch_predicted_classes.flatten() # Flatten to 1D array
})
submission_scratch_df.to_csv('submission_scratch_model.csv', index=False)
print(f"\nSubmission for Scratch Model saved to submission_scratch_model.csv:\n{submission_scratch_df.head()}")
print(f"Scratch Model Predicted Label Counts:\n{submission_scratch_df['label'].value_counts()}")





# Create submission DataFrame for Pre-trained Model
submission_pretrained_df = pd.DataFrame({
    'file_id': test_file_ids,
    'label': pretrained_predicted_classes.flatten()
})
submission_pretrained_df.to_csv('submission_pretrained_model.csv', index=False)
print(f"\nSubmission for Pre-trained Model saved to submission_pretrained_model.csv:\n{submission_pretrained_df.head()}")
print(f"Pre-trained Model Predicted Label Counts:\n{submission_pretrained_df['label'].value_counts()}")






# --- Comparison Summary ---
print("\n--- Model Comparison Summary ---")
print(f"Scratch Model Validation Accuracy: {accuracy_scratch:.4f}")
print(f"Pre-trained Model (ResNet50) Validation Accuracy: {accuracy_pretrained:.4f}")






