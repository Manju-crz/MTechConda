





import numpy as np
import pandas as pd
from pathlib import Path
import os.path
import matplotlib.pyplot as plt
import tensorflow as tf
import glob
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2

train_file_list=[]
test_file_list=[]
val_file_list=[]

for filename in glob.iglob('Dataset/train/**/*.jpg',
                           recursive = True):
    filename = filename.replace("\\", "/")
    train_file_list.append(filename)

for filename in glob.iglob('Dataset/test/**/*.jpg',
                           recursive = True):
    filename = filename.replace("\\", "/")
    test_file_list.append(filename)

for filename in glob.iglob('Dataset/validation/**/*.jpg',
                           recursive = True):
    filename = filename.replace("\\", "/")
    val_file_list.append(filename)


def proc_img(filepath):
    """ Create a DataFrame with the filepath and the labels of the pictures
    """

    labels = [str(filepath[i]).split("/")[-2] \
              for i in range(len(filepath))]

    filepath = pd.Series(filepath, name='Filepath').astype(str)
    labels = pd.Series(labels, name='Label')

    # Concatenate filepaths and labels
    df = pd.concat([filepath, labels], axis=1)

    # Shuffle the DataFrame and reset index
    df = df.sample(frac=1).reset_index(drop = True)

    return df

#A Dataframe consisting of file paths and corresponding labels has already been created for test, train and validation datasets.
train_df = proc_img(train_file_list)
test_df = proc_img(test_file_list)
val_df = proc_img(val_file_list)





import tensorflow as tf
import os

# Set dataset directory path
dataset_dir = "Dataset"

# Define paths
train_dir = os.path.join(dataset_dir, "train")
val_dir   = os.path.join(dataset_dir, "validation")
test_dir  = os.path.join(dataset_dir, "test")

# Parameters
img_height = 224
img_width = 224
batch_size = 32




# Load datasets
train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size
)



class_names = train_ds.class_names
print("Classes:", class_names)

# Optimize performance (after extracting class_names)
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds  = test_ds.cache().prefetch(buffer_size=AUTOTUNE)


for images, labels in train_ds.take(1):
    print(images.shape)
    print(labels.numpy())






normalization_layer = tf.keras.layers.Rescaling(1./255)
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds   = val_ds.map(lambda x, y: (normalization_layer(x), y))
test_ds  = test_ds.map(lambda x, y: (normalization_layer(x), y))






data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
])



train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))









from tensorflow import keras
from tensorflow.keras import layers


num_classes = len(class_names)









# ------------------------------
# Build CNN Model
# ------------------------------
model = keras.Sequential([
    layers.InputLayer(shape=(img_height, img_width, 3)),

    data_augmentation,           # apply augmentation
    normalization_layer,         # normalize pixel values

    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),         # prevent overfitting
    layers.Dense(num_classes, activation='softmax')  # output layer
])







# ------------------------------
# Compile Model
# ------------------------------
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)




# ------------------------------
# Train Model
# ------------------------------
epochs = 15
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)




# ------------------------------
# Evaluate on Test Data
# ------------------------------
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc:.4f}")






print("Number of classes:", len(class_names))
for i, class_name in enumerate(class_names):
    print(i, class_name)






base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224,224,3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # freeze base model

model = keras.Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])




# ------------------------------
# Build Transfer Learning Model
# ------------------------------
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,       # remove final classification head
    weights='imagenet'       # use pretrained weights
)
base_model.trainable = False  # freeze base model for now

model = keras.Sequential([
    data_augmentation,           # from earlier
    layers.Rescaling(1./255),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')  # output layer
])

# ------------------------------
# Compile
# ------------------------------
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ------------------------------
# Train (initial training with frozen base)
# ------------------------------
epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

# ------------------------------
# Evaluate on Test Data
# ------------------------------
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc:.4f}")






import os
for cls in class_names:
    print(cls, ":", len(os.listdir(os.path.join("dataset/train", cls))))



for images, labels in train_ds.take(1):
    plt.imshow(images[0])   # no need for astype("uint8")
    plt.title(class_names[labels[0]])
    plt.axis("off")   # optional, cleaner display
    plt.show()




acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training vs Validation Accuracy')

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training vs Validation Loss')

plt.show()



base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),   # smaller LR
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc:.4f}")



acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training vs Validation Accuracy')

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training vs Validation Loss')

plt.show()







