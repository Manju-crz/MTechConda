{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66cec9b-718f-4366-b7c2-2c7c3c5660a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1709\n",
      "Test samples: 332\n",
      "Class distribution in training data:\n",
      "label\n",
      "1    949\n",
      "0    760\n",
      "Name: count, dtype: int64\n",
      "Starting Fake vs Real Image Detection Pipeline...\n",
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'filename'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 482\u001b[0m\n\u001b[0;32m    479\u001b[0m detector \u001b[38;5;241m=\u001b[39m FakeRealImageDetector(dataset_path, test_path, train_csv_path, test_csv_path)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Run the complete pipeline\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m model_performances \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mrun_complete_pipeline()\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPipeline completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck the generated CSV files for predictions from each model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 363\u001b[0m, in \u001b[0;36mFakeRealImageDetector.run_complete_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Fake vs Real Image Detection Pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m X_train, X_val, X_test, y_train, y_val, test_filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_preprocess_data()\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Dictionary to store model performances\u001b[39;00m\n\u001b[0;32m    366\u001b[0m model_performances \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[3], line 55\u001b[0m, in \u001b[0;36mFakeRealImageDetector.load_and_preprocess_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m y_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 55\u001b[0m     filename \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m     label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Determine the correct folder based on label\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'filename'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class FakeRealImageDetector:\n",
    "    def __init__(self, dataset_path, test_path, train_csv_path, test_csv_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.test_path = test_path\n",
    "        self.train_csv_path = train_csv_path\n",
    "        self.test_csv_path = test_csv_path\n",
    "        self.img_size = (224, 224)\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 50\n",
    "        \n",
    "        # Load CSV files\n",
    "        self.train_df = pd.read_csv(train_csv_path)\n",
    "        self.test_df = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        print(f\"Training samples: {len(self.train_df)}\")\n",
    "        print(f\"Test samples: {len(self.test_df)}\")\n",
    "        print(f\"Class distribution in training data:\")\n",
    "        print(self.train_df['label'].value_counts())\n",
    "    \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load and preprocess training and test data\"\"\"\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Load training data\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        for idx, row in self.train_df.iterrows():\n",
    "            filename = row['filename']\n",
    "            label = row['label']\n",
    "            \n",
    "            # Determine the correct folder based on label\n",
    "            if label == 'real':\n",
    "                img_path = os.path.join(self.dataset_path, 'training_real', filename)\n",
    "            else:\n",
    "                img_path = os.path.join(self.dataset_path, 'training_fake', filename)\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                # Load and preprocess image\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, self.img_size)\n",
    "                    img = img.astype('float32') / 255.0\n",
    "                    \n",
    "                    X_train.append(img)\n",
    "                    y_train.append(1 if label == 'real' else 0)\n",
    "        \n",
    "        # Load test data\n",
    "        X_test = []\n",
    "        test_filenames = []\n",
    "        \n",
    "        for idx, row in self.test_df.iterrows():\n",
    "            filename = row['filename']\n",
    "            img_path = os.path.join(self.test_path, filename)\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                # Load and preprocess image\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, self.img_size)\n",
    "                    img = img.astype('float32') / 255.0\n",
    "                    \n",
    "                    X_test.append(img)\n",
    "                    test_filenames.append(filename)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        # Split training data into train and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set shape: {X_train.shape}\")\n",
    "        print(f\"Validation set shape: {X_val.shape}\")\n",
    "        print(f\"Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, test_filenames\n",
    "    \n",
    "    def create_custom_cnn_model(self):\n",
    "        \"\"\"Create a custom CNN model from scratch\"\"\"\n",
    "        model = Sequential([\n",
    "            # First Convolutional Block\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            # Second Convolutional Block\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            # Third Convolutional Block\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            # Fourth Convolutional Block\n",
    "            Conv2D(256, (3, 3), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            # Fifth Convolutional Block\n",
    "            Conv2D(512, (3, 3), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            # Flatten and Dense layers\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_vgg16_model(self):\n",
    "        \"\"\"Create a model using pre-trained VGG16\"\"\"\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_resnet50_model(self):\n",
    "        \"\"\"Create a model using pre-trained ResNet50\"\"\"\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_efficientnet_model(self):\n",
    "        \"\"\"Create a model using pre-trained EfficientNetB0\"\"\"\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_data_generators(self, X_train, y_train):\n",
    "        \"\"\"Create data generators for data augmentation\"\"\"\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        return datagen\n",
    "    \n",
    "    def train_model(self, model, X_train, X_val, y_train, y_val, model_name):\n",
    "        \"\"\"Train a model with callbacks\"\"\"\n",
    "        print(f\"\\nTraining {model_name} model...\")\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            f'{model_name}_best_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False\n",
    "        )\n",
    "        \n",
    "        # Data augmentation\n",
    "        datagen = self.create_data_generators(X_train, y_train)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            datagen.flow(X_train, y_train, batch_size=self.batch_size),\n",
    "            steps_per_epoch=len(X_train) // self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def plot_training_history(self, history, model_name):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} - Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} - Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate_model(self, model, X_val, y_val, model_name):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(f\"\\nEvaluating {model_name} model...\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_proba = model.predict(X_val)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(f\"{model_name} Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\n{model_name} Classification Report:\")\n",
    "        print(classification_report(y_val, y_pred, target_names=['Fake', 'Real']))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "        plt.title(f'{model_name} - Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def predict_test_data(self, model, X_test, test_filenames, model_name):\n",
    "        \"\"\"Make predictions on test data\"\"\"\n",
    "        print(f\"\\nMaking predictions with {model_name} model...\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Create submission dataframe\n",
    "        submission_df = pd.DataFrame({\n",
    "            'filename': test_filenames,\n",
    "            'prediction': ['real' if pred == 1 else 'fake' for pred in y_pred],\n",
    "            'confidence': y_pred_proba.flatten()\n",
    "        })\n",
    "        \n",
    "        # Save predictions\n",
    "        submission_df.to_csv(f'{model_name}_predictions.csv', index=False)\n",
    "        print(f\"Predictions saved to {model_name}_predictions.csv\")\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        print(\"Starting Fake vs Real Image Detection Pipeline...\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        X_train, X_val, X_test, y_train, y_val, test_filenames = self.load_and_preprocess_data()\n",
    "        \n",
    "        # Dictionary to store model performances\n",
    "        model_performances = {}\n",
    "        \n",
    "        # 1. Custom CNN Model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING CUSTOM CNN MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        custom_cnn = self.create_custom_cnn_model()\n",
    "        print(custom_cnn.summary())\n",
    "        \n",
    "        history_cnn = self.train_model(custom_cnn, X_train, X_val, y_train, y_val, \"Custom_CNN\")\n",
    "        self.plot_training_history(history_cnn, \"Custom CNN\")\n",
    "        \n",
    "        accuracy_cnn = self.evaluate_model(custom_cnn, X_val, y_val, \"Custom CNN\")\n",
    "        model_performances['Custom CNN'] = accuracy_cnn\n",
    "        \n",
    "        predictions_cnn = self.predict_test_data(custom_cnn, X_test, test_filenames, \"Custom_CNN\")\n",
    "        \n",
    "        # 2. VGG16 Model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING VGG16 MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        vgg16_model = self.create_vgg16_model()\n",
    "        print(vgg16_model.summary())\n",
    "        \n",
    "        history_vgg16 = self.train_model(vgg16_model, X_train, X_val, y_train, y_val, \"VGG16\")\n",
    "        self.plot_training_history(history_vgg16, \"VGG16\")\n",
    "        \n",
    "        accuracy_vgg16 = self.evaluate_model(vgg16_model, X_val, y_val, \"VGG16\")\n",
    "        model_performances['VGG16'] = accuracy_vgg16\n",
    "        \n",
    "        predictions_vgg16 = self.predict_test_data(vgg16_model, X_test, test_filenames, \"VGG16\")\n",
    "        \n",
    "        # 3. ResNet50 Model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING RESNET50 MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        resnet50_model = self.create_resnet50_model()\n",
    "        print(resnet50_model.summary())\n",
    "        \n",
    "        history_resnet50 = self.train_model(resnet50_model, X_train, X_val, y_train, y_val, \"ResNet50\")\n",
    "        self.plot_training_history(history_resnet50, \"ResNet50\")\n",
    "        \n",
    "        accuracy_resnet50 = self.evaluate_model(resnet50_model, X_val, y_val, \"ResNet50\")\n",
    "        model_performances['ResNet50'] = accuracy_resnet50\n",
    "        \n",
    "        predictions_resnet50 = self.predict_test_data(resnet50_model, X_test, test_filenames, \"ResNet50\")\n",
    "        \n",
    "        # 4. EfficientNetB0 Model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING EFFICIENTNETB0 MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        efficientnet_model = self.create_efficientnet_model()\n",
    "        print(efficientnet_model.summary())\n",
    "        \n",
    "        history_efficientnet = self.train_model(efficientnet_model, X_train, X_val, y_train, y_val, \"EfficientNetB0\")\n",
    "        self.plot_training_history(history_efficientnet, \"EfficientNetB0\")\n",
    "        \n",
    "        accuracy_efficientnet = self.evaluate_model(efficientnet_model, X_val, y_val, \"EfficientNetB0\")\n",
    "        model_performances['EfficientNetB0'] = accuracy_efficientnet\n",
    "        \n",
    "        predictions_efficientnet = self.predict_test_data(efficientnet_model, X_test, test_filenames, \"EfficientNetB0\")\n",
    "        \n",
    "        # Model Comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL COMPARISON\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Display model performances\n",
    "        performance_df = pd.DataFrame(list(model_performances.items()), \n",
    "                                    columns=['Model', 'Validation Accuracy'])\n",
    "        performance_df = performance_df.sort_values('Validation Accuracy', ascending=False)\n",
    "        \n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        print(performance_df.to_string(index=False))\n",
    "        \n",
    "        # Plot model comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(performance_df['Model'], performance_df['Validation Accuracy'])\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(performance_df['Validation Accuracy']):\n",
    "            plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Best model\n",
    "        best_model_name = performance_df.iloc[0]['Model']\n",
    "        best_accuracy = performance_df.iloc[0]['Validation Accuracy']\n",
    "        \n",
    "        print(f\"\\nBest performing model: {best_model_name}\")\n",
    "        print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "        \n",
    "        return model_performances\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths (modify these according to your folder structure)\n",
    "    dataset_path = \"dataset\"  # Contains training_real and training_fake folders\n",
    "    test_path = \"test\"        # Contains test images\n",
    "    train_csv_path = \"train.csv\"\n",
    "    test_csv_path = \"test.csv\"\n",
    "    \n",
    "    # Initialize the detector\n",
    "    detector = FakeRealImageDetector(dataset_path, test_path, train_csv_path, test_csv_path)\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    model_performances = detector.run_complete_pipeline()\n",
    "    \n",
    "    print(\"\\nPipeline completed successfully!\")\n",
    "    print(\"Check the generated CSV files for predictions from each model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a848d5d-85ba-4a58-8e6a-ecd1058cd585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92483311-2a29-4cf6-b976-21fc29775149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.10.0)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\manju\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/39.0 MB 7.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 3.7/39.0 MB 9.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.2/39.0 MB 9.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.6/39.0 MB 9.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 9.4/39.0 MB 9.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 11.3/39.0 MB 9.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.1/39.0 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.2/39.0 MB 9.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 17.0/39.0 MB 9.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.9/39.0 MB 9.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 20.7/39.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.5/39.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.6/39.0 MB 9.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.5/39.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.3/39.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.4/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.2/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 34.1/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.2/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 38.0/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.0/39.0 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.6 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.6 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.9/12.6 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.6 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp312-cp312-win_amd64.whl (223 kB)\n",
      "Installing collected packages: numpy, opencv-python, contourpy\n",
      "Successfully installed contourpy-1.3.2 numpy-2.1.3 opencv-python-4.12.0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\manju\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python tensorflow scikit-learn matplotlib seaborn pillow pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f1d31-08bd-417e-87e6-a56877a00513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
