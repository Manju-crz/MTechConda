{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b947fdb-d0ca-4516-ba94-32116a7e4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50, VGG16 # Example pre-trained models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d6ec28-24ad-4591-8a31-76a51a2acb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration and Setup ---\n",
    "# Define paths\n",
    "DATASET_DIR = 'dataset'\n",
    "TRAIN_REAL_DIR = os.path.join(DATASET_DIR, 'training_real')\n",
    "TRAIN_FAKE_DIR = os.path.join(DATASET_DIR, 'training_fake')\n",
    "TEST_DIR = 'test' # Sibling to dataset folder\n",
    "\n",
    "TRAIN_CSV_PATH = 'train.csv'\n",
    "TEST_CSV_PATH = 'test.csv'\n",
    "\n",
    "# Image parameters\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2 # Real and Fake\n",
    "EPOCHS = 20 # You might need to adjust this based on convergence\n",
    "LEARNING_RATE = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf610fff-10dd-43b1-b5ac-0ac7b0dc913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df head:\n",
      "   file_id  label\n",
      "0        0      0\n",
      "1        1      0\n",
      "2        2      0\n",
      "3        3      0\n",
      "4        4      0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1709 entries, 0 to 1708\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   file_id  1709 non-null   int64\n",
      " 1   label    1709 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 26.8 KB\n",
      "train_df info:\n",
      "None\n",
      "train_df label counts:\n",
      "label\n",
      "1    949\n",
      "0    760\n",
      "Name: count, dtype: int64\n",
      "\n",
      "test_df head:\n",
      "   file_id\n",
      "0        0\n",
      "1        1\n",
      "2        2\n",
      "3        3\n",
      "4        4\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 332 entries, 0 to 331\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   file_id  332 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 2.7 KB\n",
      "test_df info:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load train.csv\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    print(f\"train_df head:\\n{train_df.head()}\")\n",
    "    print(f\"train_df info:\\n{train_df.info()}\")\n",
    "    print(f\"train_df label counts:\\n{train_df['label'].value_counts()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {TRAIN_CSV_PATH} not found. Please ensure the CSV file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# Load test.csv\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    print(f\"\\ntest_df head:\\n{test_df.head()}\")\n",
    "    print(f\"test_df info:\\n{test_df.info()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {TEST_CSV_PATH} not found. Please ensure the CSV file is in the correct directory.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a7398-0c8a-4ca8-9ad1-fadf960eff46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab0dbaa-6300-4c3e-9700-24cf320cfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full path column for training images in train_df\n",
    "# Assuming file_id in train.csv corresponds to the image name (e.g., '0.jpg')\n",
    "# And assuming '0' is 'real' and '1' is 'fake' based on common practice for binary classification\n",
    "# We need to map file_id to the correct subdirectory (training_real or training_fake)\n",
    "# Since the train.csv only gives file_id and label, and not the source folder,\n",
    "# we need to infer the folder structure.\n",
    "# A more robust approach would be to glob all images and then match with train_df.\n",
    "\n",
    "# Let's list all files in training_real and training_fake and create a combined DataFrame\n",
    "# and then merge with train_df.\n",
    "\n",
    "# Function to get file paths and corresponding labels\n",
    "def get_image_paths_and_labels(base_dir, label_mapping):\n",
    "    data = []\n",
    "    for label_name, label_id in label_mapping.items():\n",
    "        folder_path = os.path.join(base_dir, label_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Directory {folder_path} not found. Please check your dataset structure.\")\n",
    "            continue\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')): # Filter for image files\n",
    "                data.append({'file_id': os.path.splitext(img_name)[0], 'file_path': os.path.join(folder_path, img_name), 'label': label_id})\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac3546-684a-4296-9f6e-39397473c754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd073e71-e5dc-42a1-9014-2edd32a094c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged Train DataFrame head:\n",
      "  file_id                       file_path label\n",
      "0    1000  dataset\\training_real\\1000.jpg     1\n",
      "1    1001  dataset\\training_real\\1001.jpg     1\n",
      "2    1002  dataset\\training_real\\1002.jpg     1\n",
      "3    1003  dataset\\training_real\\1003.jpg     1\n",
      "4    1004  dataset\\training_real\\1004.jpg     1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1709 entries, 0 to 1708\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_id    1709 non-null   object\n",
      " 1   file_path  1709 non-null   object\n",
      " 2   label      1709 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 40.2+ KB\n",
      "Merged Train DataFrame info:\n",
      "None\n",
      "Merged Train DataFrame label counts:\n",
      "label\n",
      "1    949\n",
      "0    760\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'training_real' corresponds to label 0 and 'training_fake' to label 1\n",
    "label_mapping = {'training_real': 0, 'training_fake': 1}\n",
    "all_train_images_df = get_image_paths_and_labels(DATASET_DIR, label_mapping)\n",
    "\n",
    "# Convert file_id to string for merging (if train_df's file_id is int)\n",
    "train_df['file_id'] = train_df['file_id'].astype(str)\n",
    "all_train_images_df['file_id'] = all_train_images_df['file_id'].astype(str)\n",
    "\n",
    "# Merge with the provided train_df to ensure correct labels based on train.csv\n",
    "# This handles cases where train.csv might contradict the folder structure (unlikely but good for robustness)\n",
    "merged_train_df = pd.merge(all_train_images_df[['file_id', 'file_path']], train_df, on='file_id', how='inner')\n",
    "merged_train_df['label'] = merged_train_df['label'].astype(str) # Convert labels to string for flow_from_dataframe\n",
    "\n",
    "print(f\"\\nMerged Train DataFrame head:\\n{merged_train_df.head()}\")\n",
    "print(f\"Merged Train DataFrame info:\\n{merged_train_df.info()}\")\n",
    "print(f\"Merged Train DataFrame label counts:\\n{merged_train_df['label'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d995e1-d5db-429c-bcc0-146cd6aa9955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23a1986-42f6-4761-a958-7924c52b8889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train split size: 1367\n",
      "Validation split size: 342\n",
      "Train split label counts:\n",
      "label\n",
      "1    759\n",
      "0    608\n",
      "Name: count, dtype: int64\n",
      "Validation split label counts:\n",
      "label\n",
      "1    190\n",
      "0    152\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if merged_train_df.empty:\n",
    "    print(\"Error: Merged training DataFrame is empty. Check file_id matching between CSV and image filenames.\")\n",
    "    exit()\n",
    "\n",
    "# Split training data for validation\n",
    "train_df_split, val_df_split = train_test_split(merged_train_df, test_size=0.2, random_state=42, stratify=merged_train_df['label'])\n",
    "\n",
    "print(f\"\\nTrain split size: {len(train_df_split)}\")\n",
    "print(f\"Validation split size: {len(val_df_split)}\")\n",
    "print(f\"Train split label counts:\\n{train_df_split['label'].value_counts()}\")\n",
    "print(f\"Validation split label counts:\\n{val_df_split['label'].value_counts()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acba0e2-4397-40d2-ae3d-1f32c3a4b074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b174a86-0dfc-4610-a364-8035893d30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Data Generators with Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5a0f31-5210-4cba-9536-d5de8fa4a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255) # No augmentation for validation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # No augmentation for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08664fb-860c-4c99-beb3-41852dcdde48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42139c53-114e-4cfb-8ea6-1ddd2cb6a68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1367 validated image filenames belonging to 2 classes.\n",
      "Found 342 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df_split,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary', # Since we have 2 classes\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df_split,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68cf6af-2200-49be-8d0c-9b332dc703bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13b2231-92f3-4efc-a8aa-6ac8c6a39aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All test files found.\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data for prediction\n",
    "# Create full paths for test images\n",
    "test_df['file_path'] = test_df['file_id'].apply(lambda x: os.path.join(TEST_DIR, str(x) + '.jpg')) # Assuming .jpg extension for test images\n",
    "\n",
    "# Verify if test image files exist\n",
    "# This helps in debugging if flow_from_dataframe fails\n",
    "missing_test_files = [path for path in test_df['file_path'] if not os.path.exists(path)]\n",
    "if missing_test_files:\n",
    "    print(f\"\\nWarning: {len(missing_test_files)} test files not found. First 5 missing: {missing_test_files[:5]}\")\n",
    "    # You might want to filter these out or handle them\n",
    "    test_df = test_df[test_df['file_path'].apply(os.path.exists)]\n",
    "    if test_df.empty:\n",
    "        print(\"Error: No valid test images found after filtering missing files.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"\\nAll test files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7635203-5ee8-4fe7-9f65-c67f15661c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b66cdb-adf8-4ea2-9577-5bd1bbaa8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 332 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='file_path',\n",
    "    y_col=None, # No labels for test set\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None, # Important for prediction\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b67a2f-4c37-4952-9e55-04531256f461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6844de4-7c2b-497f-aa88-cfb3be171e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Definition: From Scratch ---\n",
    "def create_scratch_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec31a6-8ce3-4042-977b-7952e55b5bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97856a53-d544-4ec2-b4a2-b10172822d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Model Definition: Pre-trained (Transfer Learning) ---\n",
    "\n",
    "def create_pretrained_model(base_model_name='ResNet50', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):\n",
    "    if base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported base model: {base_model_name}. Choose 'ResNet50' or 'VGG16'.\")\n",
    "\n",
    "    # Freeze the layers of the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49813ce0-13eb-4cb1-82c9-126b61529818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Training and Evaluation ---\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint_scratch = ModelCheckpoint('best_scratch_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "model_checkpoint_pretrained = ModelCheckpoint('best_pretrained_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b6394-a0f2-40f3-a7af-a891dd6a5610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc56a3a-ad43-4d9e-9f3e-66bea1a7e0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model From Scratch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manju\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,422,784</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m6,422,784\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m257\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,518,209</span> (24.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,518,209\u001b[0m (24.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,517,249</span> (24.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,517,249\u001b[0m (24.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manju\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 885ms/step - accuracy: 0.4905 - loss: 1.0960 - val_accuracy: 0.4437 - val_loss: 1.9157\n",
      "Epoch 2/20\n",
      "\u001b[1m 1/42\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 453ms/step - accuracy: 0.4062 - loss: 1.0344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manju\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.4062 - loss: 1.0344 - val_accuracy: 0.4437 - val_loss: 1.9787\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 886ms/step - accuracy: 0.5394 - loss: 0.8095 - val_accuracy: 0.4437 - val_loss: 6.9281\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.6562 - loss: 0.7157 - val_accuracy: 0.4437 - val_loss: 6.9261\n",
      "Epoch 5/20\n",
      "\u001b[1m14/42\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 774ms/step - accuracy: 0.5870 - loss: 0.7405"
     ]
    }
   ],
   "source": [
    "# --- Model 1: From Scratch ---\n",
    "print(\"\\n--- Training Model From Scratch ---\")\n",
    "scratch_model = create_scratch_model()\n",
    "scratch_model.summary()\n",
    "\n",
    "history_scratch = scratch_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, model_checkpoint_scratch]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating Scratch Model ---\")\n",
    "# Load the best saved model for evaluation\n",
    "best_scratch_model = tf.keras.models.load_model('best_scratch_model.keras')\n",
    "loss_scratch, accuracy_scratch = best_scratch_model.evaluate(validation_generator)\n",
    "print(f\"Scratch Model Validation Loss: {loss_scratch:.4f}\")\n",
    "print(f\"Scratch Model Validation Accuracy: {accuracy_scratch:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210a461-c5f4-41ab-8546-8780c0d43695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5a3a5-1e1a-406f-8745-e44fc6ee058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Training History (Scratch Model) ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_scratch.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_scratch.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Scratch Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_scratch.history['loss'], label='Train Loss')\n",
    "plt.plot(history_scratch.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Scratch Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12743e7c-9907-4e0c-8250-1225395a7b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156680c-2f6c-4fc4-9e70-60a008b49574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 2: Pre-trained (ResNet50) ---\n",
    "print(\"\\n--- Training Pre-trained Model (ResNet50) ---\")\n",
    "# Reset generators for pre-trained model (especially if batching/shuffling matters)\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "pretrained_model = create_pretrained_model(base_model_name='ResNet50')\n",
    "pretrained_model.summary()\n",
    "\n",
    "history_pretrained = pretrained_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, model_checkpoint_pretrained]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating Pre-trained Model ---\")\n",
    "# Load the best saved model for evaluation\n",
    "best_pretrained_model = tf.keras.models.load_model('best_pretrained_model.keras')\n",
    "loss_pretrained, accuracy_pretrained = best_pretrained_model.evaluate(validation_generator)\n",
    "print(f\"Pre-trained Model Validation Loss: {loss_pretrained:.4f}\")\n",
    "print(f\"Pre-trained Model Validation Accuracy: {accuracy_pretrained:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bdbfe-da37-4254-8d5e-ea4c9dc82b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a5e9e-9556-4c65-b442-c268e963c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Training History (Pre-trained Model) ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_pretrained.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_pretrained.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Pre-trained Model (ResNet50) Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_pretrained.history['loss'], label='Train Loss')\n",
    "plt.plot(history_pretrained.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Pre-trained Model (ResNet50) Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6242d20-3805-472a-86f0-6e636cb40830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf445f4-0d4a-465a-8e8d-828482e8488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Prediction on Test Set ---\n",
    "\n",
    "print(\"\\n--- Making Predictions on Test Set ---\")\n",
    "\n",
    "# Ensure test_generator is reset before prediction\n",
    "test_generator.reset()\n",
    "\n",
    "# Predictions from Scratch Model\n",
    "print(\"\\nPredicting with Scratch Model...\")\n",
    "scratch_predictions = best_scratch_model.predict(test_generator)\n",
    "scratch_predicted_classes = (scratch_predictions > 0.5).astype(int) # Convert probabilities to binary classes\n",
    "\n",
    "# Predictions from Pre-trained Model\n",
    "print(\"Predicting with Pre-trained Model...\")\n",
    "pretrained_predictions = best_pretrained_model.predict(test_generator)\n",
    "pretrained_predicted_classes = (pretrained_predictions > 0.5).astype(int)\n",
    "\n",
    "# Create submission files (assuming 'file_id' from test_df needs to be mapped to predictions)\n",
    "# We need to map the predictions back to the original file_ids.\n",
    "# test_generator.filenames contains the relative paths, we need to extract file_id.\n",
    "\n",
    "# Get the mapping of indices to original filenames (file_id.jpg) from the generator\n",
    "test_filenames = test_generator.filenames\n",
    "test_file_ids = [os.path.splitext(os.path.basename(f))[0] for f in test_filenames]\n",
    "\n",
    "# Create submission DataFrame for Scratch Model\n",
    "submission_scratch_df = pd.DataFrame({\n",
    "    'file_id': test_file_ids,\n",
    "    'label': scratch_predicted_classes.flatten() # Flatten to 1D array\n",
    "})\n",
    "submission_scratch_df.to_csv('submission_scratch_model.csv', index=False)\n",
    "print(f\"\\nSubmission for Scratch Model saved to submission_scratch_model.csv:\\n{submission_scratch_df.head()}\")\n",
    "print(f\"Scratch Model Predicted Label Counts:\\n{submission_scratch_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a53bd-47fc-4c49-82eb-319587473173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43f16c-d695-4f78-91e8-210a76f8c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame for Pre-trained Model\n",
    "submission_pretrained_df = pd.DataFrame({\n",
    "    'file_id': test_file_ids,\n",
    "    'label': pretrained_predicted_classes.flatten()\n",
    "})\n",
    "submission_pretrained_df.to_csv('submission_pretrained_model.csv', index=False)\n",
    "print(f\"\\nSubmission for Pre-trained Model saved to submission_pretrained_model.csv:\\n{submission_pretrained_df.head()}\")\n",
    "print(f\"Pre-trained Model Predicted Label Counts:\\n{submission_pretrained_df['label'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c23007-1ac7-4180-9e33-1b0a6855e729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e4f9d-fcf7-42e4-8b53-2fb67301557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison Summary ---\n",
    "print(\"\\n--- Model Comparison Summary ---\")\n",
    "print(f\"Scratch Model Validation Accuracy: {accuracy_scratch:.4f}\")\n",
    "print(f\"Pre-trained Model (ResNet50) Validation Accuracy: {accuracy_pretrained:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e05a1c-7349-4e51-b73e-dc558e83d9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1a80b-7984-4d8d-affb-6141f0f88268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
