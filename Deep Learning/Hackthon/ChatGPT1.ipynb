{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d61a61-ebbb-4023-9c65-80bb3eaa9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e03d18c8-2e12-40d9-b63a-8af22fad8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9a26e15-14cc-4d7e-9e35-f50dcb41a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TRAIN_DIR = 'dataset' \n",
    "BASE_TEST_DIR = 'test'\n",
    "TRAIN_CSV_PATH = 'train.csv'\n",
    "TEST_CSV_PATH = 'test.csv'\n",
    "SUBMISSION_FILE = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9373b84d-a74d-427a-a594-973f616d26ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07cdbd2c-59f2-4d3e-90e8-899a8e2091a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "FINE_TUNE_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc89d4f6-d8e0-40e1-a217-9d68b01f4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "def map_path(row):\n",
    "    label_folder = 'training_fake' if row['label'] == 0 else 'training_real'\n",
    "    return os.path.join(label_folder, f\"{row['file_id']}.jpg\")\n",
    "\n",
    "df['file_path'] = df.apply(map_path, axis=1)\n",
    "df['label_str'] = df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6a7fe-352d-4bbb-ac1e-7a31c952351d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a92156bf-5348-48d4-8bdb-6d162fba7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e93a48c7-bd22-4686-bb0f-7871c128c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bca96c-1d1d-47a6-bbd7-6462bc94773a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86a4b2c9-b585-4043-a7ef-151b4d953cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1367 validated image filenames belonging to 2 classes.\n",
      "Found 342 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    directory=BASE_TRAIN_DIR,\n",
    "    x_col='file_path',\n",
    "    y_col='label_str',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")\n",
    "val_gen = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    directory=BASE_TRAIN_DIR,\n",
    "    x_col='file_path',\n",
    "    y_col='label_str',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940cf2e-0118-4fe7-990c-0751485e3d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1edc25d4-94cc-422f-a5e1-d32364c324ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manju\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5696 - loss: 0.6882 - val_accuracy: 0.5556 - val_loss: 0.6896\n",
      "Epoch 2/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 884ms/step - accuracy: 0.5551 - loss: 0.6901 - val_accuracy: 0.5556 - val_loss: 0.6881\n",
      "Epoch 3/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 883ms/step - accuracy: 0.5652 - loss: 0.6883 - val_accuracy: 0.5556 - val_loss: 0.6886\n",
      "Epoch 4/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 881ms/step - accuracy: 0.5384 - loss: 0.6928 - val_accuracy: 0.5556 - val_loss: 0.6879\n",
      "Epoch 5/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 898ms/step - accuracy: 0.5547 - loss: 0.6889 - val_accuracy: 0.5556 - val_loss: 0.6876\n",
      "Epoch 6/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 895ms/step - accuracy: 0.5519 - loss: 0.6909 - val_accuracy: 0.5556 - val_loss: 0.6873\n",
      "Epoch 7/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 901ms/step - accuracy: 0.5604 - loss: 0.6858 - val_accuracy: 0.5556 - val_loss: 0.6874\n",
      "Epoch 8/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 912ms/step - accuracy: 0.5616 - loss: 0.6861 - val_accuracy: 0.5556 - val_loss: 0.6870\n",
      "Epoch 9/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 957ms/step - accuracy: 0.5649 - loss: 0.6841 - val_accuracy: 0.5556 - val_loss: 0.6870\n",
      "Epoch 10/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - accuracy: 0.5558 - loss: 0.6866 - val_accuracy: 0.5556 - val_loss: 0.6869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ad98cbc8f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=(*IMG_SIZE, 3)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bc95a-f498-408c-bbcd-a66af4124369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd4281e0-977c-4d85-b988-ca6cfc5f6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c30c409-03c9-46d7-ac99-b0197ee00a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.5330 - loss: 0.6954 - val_accuracy: 0.5556 - val_loss: 0.6886\n",
      "Epoch 2/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1s/step - accuracy: 0.5315 - loss: 0.6932 - val_accuracy: 0.5556 - val_loss: 0.6877\n",
      "Epoch 3/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5718 - loss: 0.6871 - val_accuracy: 0.5556 - val_loss: 0.6875\n",
      "Epoch 4/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5068 - loss: 0.7003 - val_accuracy: 0.5556 - val_loss: 0.6870\n",
      "Epoch 5/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5331 - loss: 0.6953 - val_accuracy: 0.5556 - val_loss: 0.6871\n",
      "Epoch 6/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5204 - loss: 0.6941 - val_accuracy: 0.5556 - val_loss: 0.6894\n",
      "Epoch 7/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5463 - loss: 0.6904 - val_accuracy: 0.5556 - val_loss: 0.6890\n",
      "Epoch 8/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.4985 - loss: 0.6995 - val_accuracy: 0.5556 - val_loss: 0.6874\n",
      "Epoch 9/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.5397 - loss: 0.6936 - val_accuracy: 0.5556 - val_loss: 0.6874\n",
      "Epoch 10/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5287 - loss: 0.6929 - val_accuracy: 0.5556 - val_loss: 0.6876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ad9a99f800>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_model = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "base_model.trainable = False  # Start frozen\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_gen, epochs=10, validation_data=val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c586b9f-c299-4f84-972e-8e9bbe1144cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 332 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manju\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step\n",
      "✅ submission.csv created!\n"
     ]
    }
   ],
   "source": [
    "# Load test file list\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "test_df['file_path'] = test_df['file_id'].astype(str) + '.jpg'\n",
    "\n",
    "# Data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_flow = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    directory=BASE_TEST_DIR,\n",
    "    x_col='file_path',\n",
    "    y_col=None,\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(test_flow)\n",
    "test_df['label'] = (preds > 0.5).astype(int)\n",
    "\n",
    "# Save CSV\n",
    "test_df[['file_id', 'label']].to_csv(SUBMISSION_FILE, index=False)\n",
    "print(\"✅ submission.csv created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187a6a9-48ba-4cfb-99ce-bd3f83dbd1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
