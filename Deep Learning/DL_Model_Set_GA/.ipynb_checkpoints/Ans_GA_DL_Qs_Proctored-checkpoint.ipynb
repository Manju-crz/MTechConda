{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a41773-db20-4979-829d-1c5ef695b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `tensorflow`\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f91620-4204-4531-8d8a-3179c23a28f3",
   "metadata": {},
   "source": [
    "Q1. Write a program to create a variable with a size of 2x3 and a constant (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e6c2fa-b79c-468f-9ab9-c2b16cbeedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x3 constant tensor\n",
    "my_constant = tf.constant([[1, 2, 3],\n",
    "                           [4, 5, 6]])\n",
    "print(my_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86300a7a-6a23-4f38-8788-f796cdd068df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7546f6ee-e5ef-4076-a984-891645bf04fa",
   "metadata": {},
   "source": [
    "Q2. Write a program to create a variable \"tf3\" with a size 3x3 and convert it to a numpy size of 3x3. Display rank of the matrix (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e6d9a2-2dd6-41ca-b54f-77b043a3178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf3 is :\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=int32, numpy=\n",
      "array([[1, 2, 3],\n",
      "       [4, 5, 6],\n",
      "       [7, 8, 9]], dtype=int32)>\n",
      "\n",
      "np_array is :\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Rank of tf3: 2\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a 3x3 TensorFlow variable\n",
    "tf3 = tf.Variable([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "print(\"tf3 is :\")\n",
    "print(tf3)\n",
    "\n",
    "np_array = tf3.numpy()\n",
    "print(\"\\nnp_array is :\")\n",
    "print(np_array)\n",
    "\n",
    "rank = tf.rank(tf3)\n",
    "print(\"\\nRank of tf3:\", rank.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895eeeb-c635-4eed-a700-7b3606d4b990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f6f601-772b-4001-acd2-a5112a1bfe69",
   "metadata": {},
   "source": [
    "Q3. Write a program to randomly generate 3x3 matrix and assign 0 to the first row first column element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8161526-1ab1-4e45-a45b-aa195948a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 3x3 Matrix:\n",
      " [[0 2 1]\n",
      " [8 1 5]\n",
      " [9 7 8]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate a random 3x3 matrix using tf.Variable (to allow assignment)\n",
    "tf_matrix = tf.Variable(tf.random.uniform(shape=(3, 3), minval=1, maxval=10, dtype=tf.int32))\n",
    "\n",
    "# Step 2: Assign 0 to the first row, first column element\n",
    "tf_matrix[0, 0].assign(0)\n",
    "\n",
    "# Step 3: Display the updated matrix\n",
    "print(\"Updated 3x3 Matrix:\\n\", tf_matrix.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0f288-ffb2-4a07-ac31-39b7204f7f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92539fef-75d9-427b-a958-fd6266cf3bc2",
   "metadata": {},
   "source": [
    "Q4. Try to identify the bug and solve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd817a38-e8b8-445e-a0f2-4fb1746ba708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manju\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step - accuracy: 0.4000 - loss: 0.6947\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4000 - loss: 0.6939\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4000 - loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4000 - loss: 0.6926\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4000 - loss: 0.6921\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4000 - loss: 0.6917\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4000 - loss: 0.6912\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4000 - loss: 0.6908\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4000 - loss: 0.6902\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4000 - loss: 0.6897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23391ea8440>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random input and output\n",
    "x = np.random.random((10, 3))               # 10 samples, 3 features\n",
    "y = np.random.randint(0, 2, (10, 1))        # 10 binary labels\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(3,)))  # First hidden layer\n",
    "model.add(layers.Dense(4, activation='relu'))                    # Second hidden layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))                 # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6c991-3e2b-46b4-bb3b-98485a646b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0415e5cb-d95c-472a-9078-76a94e91a01a",
   "metadata": {},
   "source": [
    "Q5. Write a program to create random matrices with size 4x4 and perform element wise and matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c192937-fada-4d2d-8932-857954ea0a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[5 9 4 4]\n",
      " [2 3 2 8]\n",
      " [5 3 6 3]\n",
      " [3 3 8 4]]\n",
      "\n",
      "Matrix B:\n",
      " [[5 3 4 7]\n",
      " [8 5 8 5]\n",
      " [4 3 5 5]\n",
      " [8 4 9 4]]\n",
      "\n",
      "Element-wise Multiplication (A * B):\n",
      " [[25 27 16 28]\n",
      " [16 15 16 40]\n",
      " [20  9 30 15]\n",
      " [24 12 72 16]]\n",
      "\n",
      "Matrix Multiplication (A @ B):\n",
      " [[145  88 148 116]\n",
      " [106  59 114  71]\n",
      " [ 97  60 101  92]\n",
      " [103  64 112  92]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create two random 4x4 matrices (integers between 1 and 10)\n",
    "A = tf.random.uniform(shape=(4, 4), minval=1, maxval=10, dtype=tf.int32)\n",
    "B = tf.random.uniform(shape=(4, 4), minval=1, maxval=10, dtype=tf.int32)\n",
    "\n",
    "# Step 2: Element-wise multiplication (same position elements)\n",
    "elementwise_result = tf.multiply(A, B)\n",
    "\n",
    "# Step 3: Matrix multiplication (dot product)\n",
    "matrix_result = tf.matmul(A, B)\n",
    "\n",
    "# Step 4: Display all results\n",
    "print(\"Matrix A:\\n\", A.numpy())\n",
    "print(\"\\nMatrix B:\\n\", B.numpy())\n",
    "print(\"\\nElement-wise Multiplication (A * B):\\n\", elementwise_result.numpy())\n",
    "print(\"\\nMatrix Multiplication (A @ B):\\n\", matrix_result.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716cac62-b261-4576-a405-b776d5bc84d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5*5) + (9*8) + (4*4) + (4*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ba49d-83c2-4552-811f-3f81904e1629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f93f01c0-7287-4bea-8b92-579bbfd7de21",
   "metadata": {},
   "source": [
    "6a.What will be the output feature map size for the input image of size 14X14 with filter size 5X5, stride=1, 32 filters and padding =0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd91336-5cfe-421d-a6bc-3aa44d368e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Feature Map Size: 10 x 10 x 32\n"
     ]
    }
   ],
   "source": [
    "# Given parameters\n",
    "input_height = 14\n",
    "input_width = 14\n",
    "filter_height = 5\n",
    "filter_width = 5\n",
    "stride = 1\n",
    "padding = 0\n",
    "num_filters = 32\n",
    "\n",
    "# Compute output height and width using convolution formula\n",
    "output_height = ((input_height - filter_height + 2 * padding) // stride) + 1\n",
    "output_width  = ((input_width  - filter_width  + 2 * padding) // stride) + 1\n",
    "\n",
    "# Output feature map size (height x width x channels)\n",
    "print(f\"Output Feature Map Size: {output_height} x {output_width} x {num_filters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13611919-1a80-489a-b8eb-93c8afd27d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "589fcd1c-2092-4c4a-937c-7ef6f4956137",
   "metadata": {},
   "source": [
    "6b. What will be the feature map size after max pool with filter size 2 and stride =1 (Apply this on the output of 6a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec69e64-2b65-46a3-8a45-ffd2fbc47676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Feature Map Size after Max Pooling: 9 x 9 x 32\n"
     ]
    }
   ],
   "source": [
    "# Output from Conv2D in 6a\n",
    "input_height = 10\n",
    "input_width = 10\n",
    "channels = 32\n",
    "\n",
    "# Max Pooling params\n",
    "pool_size = 2\n",
    "stride = 1\n",
    "\n",
    "# Apply formula\n",
    "output_height = ((input_height - pool_size) // stride) + 1\n",
    "output_width  = ((input_width - pool_size)  // stride) + 1\n",
    "\n",
    "print(f\"Output Feature Map Size after Max Pooling: {output_height} x {output_width} x {channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40c380-3f4b-42a4-8ed5-8440d41c99b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e8929f-33a4-47a4-a705-5d324720e818",
   "metadata": {},
   "source": [
    "6c. What will be the parameters in pooling layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44d478-0dc3-468b-a637-486d475930c1",
   "metadata": {},
   "source": [
    "pool_size = (2, 2)\n",
    "strides = (1, 1)\n",
    "padding = 'valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2c19a-391f-4d27-afc3-86f25bfddae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467a9721-6ac7-4a6c-abd6-eb0a065cbc6c",
   "metadata": {},
   "source": [
    "6d. How many paramters do we have after the fully connected layer with 400 neurons? (Apply this on the output of 6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4861eb8a-9c3e-49f8-9848-46adbe1660b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2592</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,037,200</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2592\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                 │       \u001b[38;5;34m1,037,200\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,037,200</span> (3.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,037,200\u001b[0m (3.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,037,200</span> (3.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,037,200\u001b[0m (3.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "input_layer = tf.keras.Input(shape=(9, 9, 32))\n",
    "x = tf.keras.layers.Flatten()(input_layer)\n",
    "x = tf.keras.layers.Dense(400)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee8d2a-758b-4c39-b84d-ce49c167af33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613185ed-a52e-42b6-95b1-227431701489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d1cd4a-3fa4-4d16-beaf-c3082240f50f",
   "metadata": {},
   "source": [
    "Q7. What are the advantages of Maxpool and 1X1 convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ccc2b-450b-44ae-b188-5bc59d039df6",
   "metadata": {},
   "source": [
    "Max Pooling is a downsampling operation that selects the maximum value from a region.\n",
    "| Advantage                              | Explanation                                                                     |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| 🔻 **Reduces spatial size**            | Shrinks feature maps (e.g., 28×28 → 14×14), reducing computation                |\n",
    "| 🧠 **Captures dominant features**      | Keeps only the **strongest activation** (most important feature) in each region |\n",
    "| 🎯 **Provides translation invariance** | Slight shifts in input don’t affect the output drastically                      |\n",
    "| 🧹 **Removes noise**                   | By discarding weaker activations, it filters out less useful signals            |\n",
    "| ⚡ **Speeds up training**               | Fewer parameters and smaller feature maps → faster computation                  |\n",
    "\n",
    "Advantages of 1×1 Convolution\n",
    "| Advantage                            | Explanation                                                                            |\n",
    "| ------------------------------------ | -------------------------------------------------------------------------------------- |\n",
    "| 🎛 **Channel-wise transformation**   | Allows mixing/compressing channels without affecting spatial resolution                |\n",
    "| 📉 **Dimensionality reduction**      | Reduces number of channels (e.g., from 256 → 64), lowering computation                 |\n",
    "| 🧱 **Adds non-linearity**            | Usually followed by activation (ReLU), so it increases model capacity                  |\n",
    "| ⚙️ **Computational efficiency**      | Much cheaper than 3x3 or 5x5 convolutions                                              |\n",
    "| 🏗️ **Used in modern architectures** | Key in **Inception**, **ResNet**, **MobileNet** for building deep yet efficient models |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a4352-e2b8-4829-a695-e3407c3a8fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2aafbf5-29c3-4eb4-ae55-adf5d581f0d3",
   "metadata": {},
   "source": [
    "Q8. What will be the loss function for binary class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b587893-51ca-4451-b314-feefd14af848",
   "metadata": {},
   "source": [
    "Loss=−[y⋅log(p)+(1−y)⋅log(1−p)]\n",
    "y = true label (0 or 1)\n",
    "p = predicted probability of class 1 (from your model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca1857-dd5e-4a2f-9f1b-bf71a89d62c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2a2b6-0534-49ff-bd17-f14bdcf8070e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73792b9f-ac1d-406d-b6c0-140face99540",
   "metadata": {},
   "source": [
    "9a. What are the various versions of YOLO networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce0d5f-6fc4-4ab5-b045-f96a4f1a1831",
   "metadata": {},
   "source": [
    "| Version | Year | Highlights                          | Developer   |\n",
    "| ------- | ---- | ----------------------------------- | ----------- |\n",
    "| YOLOv1  | 2016 | Fastest, basic grid-based detection | Redmon      |\n",
    "| YOLOv2  | 2017 | Anchor boxes, YOLO9000              | Redmon      |\n",
    "| YOLOv3  | 2018 | Multi-scale, residuals, Darknet-53  | Redmon      |\n",
    "| YOLOv4  | 2020 | CSPDarknet, many improvements       | Bochkovskiy |\n",
    "| YOLOv5  | 2020 | PyTorch, industry-friendly          | Ultralytics |\n",
    "| YOLOv6  | 2022 | Optimized for deployment            | Meituan     |\n",
    "| YOLOv7  | 2022 | Best accuracy-speed trade-off       | Wang et al. |\n",
    "| YOLOv8  | 2023 | Modular, supports many tasks        | Ultralytics |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b58334-7fef-4416-a6df-2c60bbbb4ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e700c39-b847-4106-8aec-8573d414150a",
   "metadata": {},
   "source": [
    "9b. What is the significant difference between YOLO and YOLOV3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd54956-f6de-42a7-82d8-01f7c595c508",
   "metadata": {},
   "source": [
    "| Feature / Aspect              | **YOLO (v1)**                        | **YOLOv3**                                               |\n",
    "| ----------------------------- | ------------------------------------ | -------------------------------------------------------- |\n",
    "| 📅 **Year Introduced**        | 2016                                 | 2018                                                     |\n",
    "| 🧠 **Architecture**           | Shallow CNN, \\~24 conv layers + 2 FC | Deep CNN: **Darknet-53** (53 conv layers with residuals) |\n",
    "| 📏 **Detection Scale**        | Single-scale detection               | **Multi-scale detection** (detects at 3 different sizes) |\n",
    "| 📦 **Bounding Boxes**         | Single box per grid cell             | Multiple anchor boxes per cell (like Faster R-CNN)       |\n",
    "| 🔁 **Residual Connections**   | ❌ Not used                           | ✅ **Used**, inspired by ResNet                           |\n",
    "| 🧮 **Class Prediction**       | Uses softmax classification          | Uses **independent logistic classifiers** (sigmoid)      |\n",
    "| 🎯 **Small Object Detection** | Poor performance for small objects   | ✅ Significantly better with multi-scale prediction       |\n",
    "| 🛠️ **Post-Processing**       | Basic NMS                            | Improved NMS with better IOU thresholds                  |\n",
    "| ⚡ **Speed vs Accuracy**       | Extremely fast but less accurate     | Balanced: **high accuracy + decent speed**               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdece746-fa25-4e3d-a0c0-826ab53a3c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be3de504-39c4-4fa1-8dd1-8a39f84a8859",
   "metadata": {},
   "source": [
    "9c. What are advantages of YOLO algorithm with RCNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e397005-e3d3-45d7-aea6-dd8823cec7e5",
   "metadata": {},
   "source": [
    "| Advantage                   | YOLO (You Only Look Once)                             | R-CNN (Region-based CNN)                                     |\n",
    "| --------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| ⚡ **Speed**                 | **Real-time** detection (e.g., 45–150 FPS)            | Very **slow** (R-CNN ≈ 0.2 FPS, Fast R-CNN ≈ 5 FPS)          |\n",
    "| 🧠 **Single Unified Model** | YOLO is **end-to-end** trained as a single network    | R-CNN uses **multi-stage pipeline** (region proposals + CNN) |\n",
    "| 🔧 **Simple Architecture**  | No region proposal step; direct detection in one pass | Complex architecture: Selective Search + CNN + SVM + NMS     |\n",
    "| 📦 **Global Reasoning**     | YOLO looks at the **entire image at once**            | R-CNN focuses on **local regions**, may miss context         |\n",
    "| 🚀 **Deployment Friendly**  | Lightweight; easier to deploy on edge/mobile devices  | Heavier and more complex to deploy                           |\n",
    "| 🤖 **Unified Inference**    | Detection is a **single network prediction** pass     | Requires **running CNN per region proposal** (very slow)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700408d-a42b-452d-b386-d16454632de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39fc3f40-5c13-411d-bd5f-f20f14dc0ad4",
   "metadata": {},
   "source": [
    "9d. What are the dense sampling methods? How are they differnet from region based methods? What will be output dimension volume for a 19X19 image? (9 Bounding boxes, 6 classes with no background). What will be the output dimension for no object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6bd0a5-517c-4d34-81f6-2416496013ed",
   "metadata": {},
   "source": [
    "1. What are Dense Sampling Methods?\n",
    "🔍 Dense Sampling:\n",
    "In dense sampling, the entire image is scanned in a sliding-window fashion, generating predictions at regular intervals across the whole spatial grid.\n",
    "\n",
    "Every location on the grid is forced to make predictions (whether an object is present or not).\n",
    "\n",
    "Used in models like YOLO, SSD, and Fully Convolutional Networks (FCNs).\n",
    "\n",
    "Example:\n",
    "In YOLO, an image is divided into a grid (say, 19×19), and every cell predicts multiple bounding boxes regardless of whether there is an object or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79a28d-f092-428b-80b0-0b3b71a5d8e6",
   "metadata": {},
   "source": [
    "2. How are Dense Sampling Methods Different from Region-Based Methods?\n",
    "| Feature                    | **Dense Sampling**                           | **Region-Based Methods (like R-CNN)**               |\n",
    "| -------------------------- | -------------------------------------------- | --------------------------------------------------- |\n",
    "| 🔍 Region Proposal         | Not needed — predict for **every grid cell** | Uses **selective search or RPN** to propose regions |\n",
    "| ⚙️ Architecture Simplicity | Simpler, end-to-end trainable                | Multi-stage (region proposal + classification)      |\n",
    "| ⚡ Speed                    | Much faster (real-time capable)              | Slower (especially R-CNN, Fast R-CNN)               |\n",
    "| ❌ False Positives          | Higher chance (makes predictions everywhere) | More precise region focus                           |\n",
    "| 🧠 Global Context          | Looks at the whole image                     | Focuses on selected regions                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8de590-0f9f-4d5b-bad7-97ad07500746",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Output Dimension Volume for a 19×19 Grid\n",
    "Given:\n",
    "\n",
    "Grid = 19×19\n",
    "\n",
    "Bounding boxes per grid cell = 9\n",
    "\n",
    "Classes = 6 (No background class explicitly)\n",
    "\n",
    "Each bounding box predicts:\n",
    "\n",
    "4 values for box coords (x, y, w, h)\n",
    "\n",
    "1 value for objectness score\n",
    "\n",
    "6 class probabilities (total = 6)\n",
    "\n",
    "Per Bounding Box:\n",
    "Output per box=4(box coords)+1(objectness)+6(classes)= \n",
    "11\n",
    "Final Output Volume:\n",
    "Output Shape=19×19×(9×11)= 19×19×99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828692f2-0e44-4a87-bc64-d356ed1e13ec",
   "metadata": {},
   "source": [
    "4. Output Dimension for No Object\n",
    "Even if there's no object, dense sampling networks like YOLO still produce the same output shape, i.e.:\n",
    "19×19×99\n",
    "| Question                                       | Answer                                       |\n",
    "| ---------------------------------------------- | -------------------------------------------- |\n",
    "| What are dense sampling methods?               | Predicts bounding boxes at all grid cells    |\n",
    "| Difference from region-based methods           | No region proposal, much faster, global view |\n",
    "| Output dimension for 19×19, 9 boxes, 6 classes | `19 × 19 × 99`                               |\n",
    "| Output when no object                          | Still `19 × 19 × 99`, with low objectness    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2caa73-b34a-46c7-8d0e-b5d00167d5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f489b98-1251-4e02-950a-00179bb61e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d99c22-9183-4924-9cc6-1d325665885d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e64b8-070c-41dd-a7e1-311e5d5e4c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08fb77-80c1-4989-ae1e-ff6518e7d710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84223ce4-7f57-43c9-a030-07d97cfe1806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
